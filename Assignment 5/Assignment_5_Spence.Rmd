---
title: "Assignment 4 - Customer Rating of Breakfast Cereals"
author: "Steve Spence"
date: "11/14/2019"
output: word_document
---

## Load Data Set and Libraries

First, we will load all of the packages that will be required for this problem. Specifically, "ISLR", "caret", "dplyr", "tidyverse", "factoextra", "ggplot2", "proxy", "NbClust", "ppclust", "dendextend", and "cluster" will be loaded for this problem.

```{r include=FALSE}

# Require all the packages that will be used in this problem

require(ISLR)
require(caret)
require(dplyr)
require(tidyverse)
require(cluster)
require(factoextra)
require(ggplot2)
require(proxy)
require(NbClust)
require(ppclust)
require(dendextend)

```

Next, we will import the "cereal" data set into the RStudio environment.

```{r}

# Import data set from BlackBoard into the RStudio environment

cereal <- read.csv("cereal.csv")

```

## Review Data Structure

A summary of the data set will be displayed to review the data set.

```{r}

# Review first few rows of the data set

head(cereal)

# Investigate the structure of the data set

str(cereal)

# Investigate the summary of the data set

summary(cereal)

```

## Data Preprocessing

The data will be scaled prior to removing the NA values from the data set.

```{r}

# Create duplicate of data set for preprocessing

cereal_scaled <- cereal

# Scale the data set prior to placing it into a clustering algorithm

cereal_scaled[ , c(4:16)] <- scale(cereal[ , c(4:16)])

# Remove NA values from data set

cereal_preprocessed <- na.omit(cereal_scaled)
  
# Review the scaled data set with NA's removed

head(cereal_preprocessed)

```

After pre-processing and scaling the data, the total number of observations went from 77 to 74. Therefore, there were only 3 records with an "NA" value.

## Assignment Task A

"Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from  single linkage, complete linkage, average linkage, and Ward. Choose the best method."

Single Linkage:

```{r}

# Create the dissimilarity matrix for the numeric values in the data set via Euclidean distance measurements

cereal_d_euclidean <- dist(cereal_preprocessed[ , c(4:16)], method = "euclidean")

# Perform hierarchical clustering via the single linkage method

ag_hc_single <- agnes(cereal_d_euclidean, method = "single")

# Plot the results of the different methods

plot(ag_hc_single, 
     main = "Customer Cereal Ratings - AGNES - Single Linkage Method",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55,
     hang = -1)

```

Complete Linkage:


```{r}

# Perform hierarchical clustering via the complete linkage method

ag_hc_complete <- agnes(cereal_d_euclidean, method = "complete")

# Plot the results of the different methods

plot(ag_hc_complete, 
     main = "Customer Cereal Ratings - AGNES - Complete Linkage Method",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55,
     hang = -1)

```

Average Linkage:

```{r}

# Perform hierarchical clustering via the average linkage method

ag_hc_average <- agnes(cereal_d_euclidean, method = "average")

# Plot the results of the different methods

plot(ag_hc_average, 
     main = "Customer Cereal Ratings - AGNES - Average Linkage Method",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55,
     hang = -1)

```

Ward Method:

```{r}

# Perform hierarchical clustering via the ward linkage method

ag_hc_ward <- agnes(cereal_d_euclidean, method = "ward")

# Plot the results of the different methods

plot(ag_hc_ward, 
     main = "Customer Cereal Ratings - AGNES - Ward Linkage Method",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55,
     hang = -1)

```

The best clustering method would be based on the agglomerative coefficient that is returned from each method. The close the value is to 1.0, the closer the clustering structure is. Therefore, the method with the value closest to 1.0 will be chosen.

Single Linkage: 0.61
Complete Linkage: 0.84
Average Linkage: 0.78
Ward Method: 0.90

As a result, the Ward method will be chosen as the best clustering model in this problem.

## Assignment Task B

"How many clusters would you choose?"

To determine the appropriate number of clusters, we will use the elbow and silhouette methods.

Elbow Method:

```{r}

# Determine the optimal number of clusters for the dataset via the Elbow method

fviz_nbclust(cereal_preprocessed[ , c(4:16)], hcut, method = "wss", k.max = 25) +
  labs(title = "Optimal Number of Clusters - Elbow Method") +
  geom_vline(xintercept = 12, linetype = 2)

```

Silhouette Method:

```{r}

# Determine the optimal number of clusters for the dataset via the silhouette method

fviz_nbclust(cereal_preprocessed[ , c(4:16)], 
                               hcut, 
                               method = "silhouette", 
                               k.max = 25) +
  labs(title = "Optimal Number of Clusters - Silhouette Method")

```

Based on the agreement of the silhouette and elbow method, the appropriate number of clusters would be 12 in this case.

Below we will outline the 12 clusters on the hierarchical tree

```{r}

# Plot of the Ward hierarchical tree with the 12 clusters outlined for reference

plot(ag_hc_ward, 
     main = "AGNES - Ward Linkage Method - 12 Clusters Outlined",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55,
     hang = -1)
rect.hclust(ag_hc_ward, k = 12, border = 1:12)

```

## Assignment Task C

"Comment on the structure of the clusters and on their stability. Hint: To check stability,  partition the data and see how well clusters formed based on one part apply to the other part. To do this:
1. Cluster partition A
2. Use the cluster centroids from A to assign each record in partition B (each record is assigned to the cluster with the closest centroid).
3. Assess how consistent the cluster assignments are compared to the assignments based on all the data"

All Data Assigned Clusters:

The assigned clusters for all data sets will be in "cereal_preprocessed_1":

```{r}

# Cut the tree into 12 clusters for analysis

ward_clusters_12 <- cutree(ag_hc_ward, k = 12)

# Add the assigned cluster to the preprocessed data set

cereal_preprocessed_1 <- cbind(cluster = ward_clusters_12, cereal_preprocessed)

```

Partition Data:

To check stability of clusters, the data set will be split into a 70/30 partition. The 70% will be used to create cluster assignments again, and then the remaining 30% will be assigned based on their closest centroid.

```{r}

# Set the seed for randomized functions

set.seed(111319)

# Split the data into 70% partition A and 30% partition B

cerealIndex <- createDataPartition(cereal_preprocessed$protein, p=0.3, list = F)

cereal_preprocessed_PartitionB <- cereal_preprocessed[cerealIndex, ]

cereal_preprocessed_PartitionA <- cereal_preprocessed[-cerealIndex,] 

```

Re-Run Clustering with Partitioned Data:

For the purposes of this task, we will assume the same K value (12) and ward clustering method to determine the stability of the clusters. We will then assign clusters to the nearest points in Partition B (for clusters 1 to 12).

```{r}

# Create the dissimilarity matrix for the numeric values in the partitioned data set via Euclidean distance measurements

cereal_d_euclidean_A <- dist(cereal_preprocessed_PartitionA[ , c(4:16)], method = "euclidean")

# Perform hierarchical clustering via the ward linkage method on partitioned data

ag_hc_ward_A <- agnes(cereal_d_euclidean_A, method = "ward")

# Plot the results of the different methods

plot(ag_hc_ward_A, 
     main = "Customer Cereal Ratings - Ward Linkage Method - Partition A",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55,
     hang = -1)

# Cut the tree into 12 clusters for analysis

ward_clusters_12_A <- cutree(ag_hc_ward_A, k = 12)

# Add the assigned cluster to the preprocessed data set

cereal_preprocessed_A <- cbind(cluster = ward_clusters_12_A, cereal_preprocessed_PartitionA)

```

The centroids for each of the clusters will need to be calculated, so we can find the closest centroid for the data points in partition B.

```{r}

# Find the centroids for the re-ran Ward hierarchical clustering

ward_Centroids_A <- aggregate(cereal_preprocessed_A[ , 5:17], list(cereal_preprocessed_A$cluster), mean)

ward_Centroids_A <- data.frame(Cluster = ward_Centroids_A[ , 1], Centroid = rowMeans(ward_Centroids_A[ , -c(1:4)]))

ward_Centroids_A <- ward_Centroids_A$Centroid

```

```{r}

# Calculate Centers of Partition B data set

cereal_preprocessed_PartitionB_centers <- data.frame(cereal_preprocessed_PartitionB[, 1:3], Center = rowMeans(cereal_preprocessed_PartitionB[ , 4:16]))

```

```{r}

# Calculate the distance between the centers of partition A and the values of partition B

B_to_A_centers <- dist(ward_Centroids_A, cereal_preprocessed_PartitionB_centers$Center, method = "euclidean")

# Assign the clusters based on the minimum distance to cluster centers

cereal_preprocessed_B <- cbind(cluster = c(4,8,7,3,5,6,7,11,11,10,8,5,10,1,10,1,4,12,12,7,7,1,4,9), cereal_preprocessed_PartitionB)

# Combine partitions A and B for comparision to original clusters

cereal_preprocessed_2 <- rbind(cereal_preprocessed_A, cereal_preprocessed_B)

cereal_preprocessed_1 <- cereal_preprocessed_1[order(cereal_preprocessed_1$name), ]
cereal_preprocessed_2 <- cereal_preprocessed_2[order(cereal_preprocessed_2$name), ]

```

Now that the data has been assigned by both methods (full data and partitioned data), we can compare the number of matching assignments to see the stability of the clusters.

```{r}

sum(cereal_preprocessed_1$cluster == cereal_preprocessed_2$cluster)


```

From this result, it can be stated that the clusters are not very stable. With 70% of the data available, the resulting assignments were only identical for 35 out of the 74 observations. This results in a 47% repeatability of assignment.

```{r}

# Visualize the cluster assignments to see any difference between the two

# Plot of original hierarchical clustering algorithm

ggplot(data = cereal_preprocessed_1, aes(cereal_preprocessed_1$cluster)) +
  geom_bar(fill = "blue4") +
  labs(title="Count of Cluster Assignments - All Original Data") +
  labs(x="Cluster Assignment", y="Count") +
  guides(fill=FALSE) +
  scale_x_continuous(breaks=c(1:12)) +
  scale_y_continuous(breaks=c(5,10,15,20), limits = c(0,25))

# Plot of algorithm that was partitioned prior to assigning the remaining data

ggplot(data = cereal_preprocessed_2, aes(cereal_preprocessed_2$cluster)) +
  geom_bar(fill = "blue4") +
  labs(title="Count of Cluster Assignments - Partitioned Data") +
  labs(x="Cluster Assignment", y="Count") +
  guides(fill=FALSE) +
  scale_x_continuous(breaks=c(1:12)) +
  scale_y_continuous(breaks=c(5,10,15,20), limits = c(0,25))

```

Visually, we can see that Cluster 3 significantly shrunk when using the partitioned data. As a result, several of the other clusters became larger as a result. From the chart, it appears the clusters are more evenly distributed across the 12 clusters when the data is partitioned.

## Assignment Task D

"The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?"

In this case, normalizing the data would not be appropriate. It would not be appropriate, because the scaling/normalizing of the cereal nutritional information is based on the sample of cereal being analyzed. Therefore, the gathered dataset could include only cereals with very high sugar content and very low fiber, iron, and other nutrional information. Once it is scaled/normalized across the sample set, it is impossible to state how much nutrition the cereal will give a child. An uninformed viewer, may assume a cereal with 0.999 for iron would mean it has almost all of the nutrional iron a child needs; however, it may just be the best of the worst in the sample set (having nearly no nutrional value).

As a result, a more appropriate means for preprocessing the data would be to make it a ratio to the daily recommended calories, fiber, carbohydrates, etc. for a child. This would allow analysts to make better informed decision about the clusters when reviewing, but not allow a few larger variables to overtake the distance calculations. When reviewing the clusters, an analyst could review the average for the cluster to determine what percentage of a students daily recommended nutrion would come from XX cereal. This would allow the staff to make informaed decisions about what the "healthy" cereal clusters to pick from are.

